{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7h0mK4Wg7aw4"
   },
   "source": [
    "### Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtJApIy375_B",
    "outputId": "5b15891c-4bad-4ff1-a306-8a74b8388401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q langchain_community\n",
    "!pip install -q sentence_transformers\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeXgHqSp7-_B"
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F3cbHyRc78hh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model= \"model\\llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                    model_type= 'llama',\n",
    "                    config={'max_new_tokens': 600,\n",
    "                              'temperature': 0.01,\n",
    "                              'context_length': 5000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Unterscheidung between RAG and Agile?\\nRAG stands for \"Risks, Assumptions, and Gates\". It is a tool used in project management to identify, track, and manage risks, assumptions, and milestones in a project. RAG status is typically used in conjunction with Agile methodologies, but it can also be used in other project management frameworks.\\n\\nRAG is often used in Agile projects to help teams prioritize and manage their work. It provides a simple and visual way to categorize tasks based on their level of risk or uncertainty. Tasks are assigned a RAG status, which can be either Green (low risk), Amber (medium risk), or Red (high risk). This helps teams identify the most critical tasks and allocate resources accordingly.\\n\\nHere are some key differences between RAG and Agile:\\n\\n1. Focus: RAG is focused specifically on risk management, while Agile is a broader project management framework that encompasses various aspects of project delivery, including planning, execution, and monitoring.\\n2. Scope: RAG is typically used in conjunction with Agile methodologies, whereas Agile can be used independently or in combination with other frameworks.\\n3. Level of detail: RAG provides a high level of detail regarding the risk management process, while Agile offers a more general framework for project delivery.\\n4. Emphasis: RAG places a strong emphasis on identifying and managing risks, whereas Agile emphasizes collaboration, flexibility, and continuous improvement.\\n5. Tools and techniques: RAG utilizes specific tools and techniques for risk management, such as risk registers and risk assessment matrices, while Agile employs various techniques for project delivery, including sprint planning, daily stand-ups, and retrospectives.\\n6. Timeframe: RAG is typically used on a shorter timeframe than Agile, focusing on the immediate risks and assumptions associated with a specific project or phase of work, whereas Agile can be applied to longer-term projects or entire product development cycles.\\n7. Team structure: RAG often involves a dedicated risk management team, while Agile typically relies on cross-functional teams composed of various stakeholders and subject matter experts.\\n8. Decision-making: RAG involves more formalized decision-making processes than Agile, which tends to be more flexible and adaptive in nature.\\n9. Metrics and reporting: RAG typically includes specific metrics and reporting requirements for risk management, while Agile emphasizes continuous monitoring and feedback through various performance metrics, such as velocity and lead time.\\n10. Cultural fit: RAG is often seen as a more structured and formalized approach to project management, whereas Agile is generally viewed as a more flexible and adaptive methodology'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is RAG??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQxF10qT-UXc"
   },
   "source": [
    "### using **custom** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBNlpm8__Tsx"
   },
   "source": [
    "#### RecursiveCharacterTextSplitter is a text splitter that splits the text into chunks, trying to keep paragraphs togeher and avoid loosing context over pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3PY9wEh--WVQ"
   },
   "outputs": [],
   "source": [
    "pdf_reader = PyPDFLoader(\"RAGPaper.pdf\")\n",
    "documents = pdf_reader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iEYY47mb_fnQ"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(api_key=HF_TOKEN,\n",
    "                                               model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "db = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "# FAISS: Facebook AI Similarity Search --> Powerful library for similarity search and clustering of dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT=\"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
    "answering something not correct. If you don't know the answer to a question,\n",
    "please don't share false information. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow up Input: {question}\n",
    "Standalone questions: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\n",
    "template = B_INST + SYSTEM_PROMPT + instruction + E_INST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate(template=template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "S6Va-LF7-1BC"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,retriever=db.as_retriever(),condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(combine_docs_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=CTransformers(client=<ctransformers.llm.LLM object at 0x0000015558E9F9D0>, model='model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', config={'max_new_tokens': 600, 'temperature': 0.01, 'context_length': 5000})), document_variable_name='context'), question_generator=LLMChain(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of \\nanswering something not correct. If you don't know the answer to a question,\\nplease don't share false information. \\n<</SYS>>\\n\\n\\nGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\nChat History:\\n{chat_history}\\nFollow up Input: {question}\\nStandalone questions: \\n[/INST]\"), llm=CTransformers(client=<ctransformers.llm.LLM object at 0x0000015558E9F9D0>, model='model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', config={'max_new_tokens': 600, 'temperature': 0.01, 'context_length': 5000})), return_source_documents=True, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInferenceAPIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000015523853370>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNsB9gehC7Gi"
   },
   "source": [
    "### Ask a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0im4CUd1CDuT",
    "outputId": "920306ab-90ad-4c98-b809-cbbf165e1e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sachin Ramesh Tendulkar (born April 24, 1973) is a former Indian cricketer and captain who is widely regarded as one of the greatest batsmen in the history of cricket. He was born in Mumbai, India, and made his first-class debut in 1984. Tendulkar scored over 34,000 runs in international cricket, including 15,921 runs in Test cricket, which is the most by any player in history. He also holds several other records, including most centuries scored in Test cricket (51) and most runs scored in a single innings of a Test match (248). Tendulkar was named the ICC Cricketer of the Year in 1997 and 2010, and he was awarded the Bharat Ratna, India's highest civilian honor, in 2014.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"Who is Sachin Tendulkar\"\"\"\n",
    "result = qa({\"question\":query,\"chat_history\":chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUgPaRiJEN5H",
    "outputId": "1ec5a76d-36cf-4cf2-98dc-0c77da67156c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAGs stands for Retrieval-based Autoencoder with Generative modeling. It's a type of neural network architecture that combines the strengths of autoencoders and generative models to perform various NLP tasks such as text generation, language translation, and question answering.\n",
      "\n",
      "RAGs consist of two main components: a retriever and a generator. The retriever takes in a query and outputs a distribution over text passages, which can be used to retrieve relevant documents or sections of a document. The generator then takes the retrieved documents or sections as additional context and generates the target sequence.\n",
      "\n",
      "One of the main use cases of RAGs is for language translation tasks. By retrieving relevant documents or sections of a document in the source language, the generator can learn to generate the target language text more accurately. This can be particularly useful when the target language has limited data available for training.\n",
      "\n",
      "Another use case of RAGs is for question answering tasks. By retrieving relevant documents or sections of a document that answer similar questions, the generator can learn to generate answers to new questions more accurately. This can be particularly useful when the questions are complex or require a deep understanding of the context.\n",
      "\n",
      "RAGs have also been used for text generation tasks such as summarization and creative writing. By retrieving relevant documents or sections of a document, the generator can learn to generate new text that is similar in style and content to the original documents.\n",
      "\n",
      "In summary, RAGs are a powerful tool for NLP tasks that require both contextual understanding and generation capabilities. They have been used successfully in language translation, question answering, and text generation tasks, among others.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"What is RAGs and tell me more about use cases of RAGs, in a detailed manner\"\"\"\n",
    "result = qa.invoke({\"question\":query,\"chat_history\":chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
