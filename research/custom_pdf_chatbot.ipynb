{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7h0mK4Wg7aw4"
   },
   "source": [
    "### Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtJApIy375_B",
    "outputId": "5b15891c-4bad-4ff1-a306-8a74b8388401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q langchain_community\n",
    "!pip install -q sentence_transformers\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeXgHqSp7-_B"
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F3cbHyRc78hh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model= \"../model\\llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                    model_type= 'llama',\n",
    "                    config={'max_new_tokens': 600,\n",
    "                              'temperature': 0.01,\n",
    "                              'context_length': 5000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b354c4729c2b4d688917a4ab0a3e8776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1ca1521fc747f183dc368f0709f87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce60f60488cc44e5a1336b4e0ed3a05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89187d9be8c4e628d4b6b08d6ec6e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f765ff4332b34535888689e3d875a24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb3b9b91730486e9340392db1d62d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b055b987a12a4e32b578971bda4a665a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe7bedb8c084e248bbe514add7cd63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9ce2c98ea248b0a6ae48a652b33484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4934cad9023a49b3a88e4662ddd107ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfa4ac8e3c642e8a7176e1ab340b408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-base-en-v1.5', cache_folder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path= \"../models--BAAI--bge-base-en-v1.5\\\\snapshots\\\\a5beb1e3e68b9ab74eb54cfd186867f64f240e1a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with model configuration options, specifying to use the CPU/GPU for computations\n",
    "model_kwargs = {'device':'cuda'} #model_kwargs = {'device':'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Unterscheidung between RAG and Agile?\\nRAG stands for \"Risks, Assumptions, and Gates\". It is a tool used in project management to identify, track, and manage risks, assumptions, and milestones in a project. RAG status is typically used in conjunction with Agile methodologies, but it can also be used in other project management frameworks.\\n\\nRAG is often used in Agile projects to help teams prioritize and manage their work. It provides a simple and visual way to categorize tasks based on their level of risk or uncertainty. Tasks are assigned a RAG status, which can be either Green (low risk), Amber (medium risk), or Red (high risk). This helps teams identify the most critical tasks and allocate resources accordingly.\\n\\nHere are some key differences between RAG and Agile:\\n\\n1. Focus: RAG is focused specifically on risk management, while Agile is a broader project management framework that encompasses various aspects of project delivery, including planning, execution, and monitoring.\\n2. Scope: RAG is typically used in conjunction with Agile methodologies, whereas Agile can be used independently or in combination with other frameworks.\\n3. Level of detail: RAG provides a high level of detail regarding the risk management process, while Agile offers a more general framework for project delivery.\\n4. Emphasis: RAG places a strong emphasis on identifying and managing risks, whereas Agile emphasizes collaboration, flexibility, and continuous improvement.\\n5. Tools and techniques: RAG utilizes specific tools and techniques for risk management, such as risk registers and risk assessment matrices, while Agile employs various techniques for project delivery, including sprint planning, daily stand-ups, and retrospectives.\\n6. Timeframe: RAG is typically used on a shorter timeframe than Agile, focusing on the immediate risks and assumptions associated with a specific project or phase of work, whereas Agile can be applied to longer-term projects or entire product development cycles.\\n7. Team structure: RAG often involves a dedicated risk management team, while Agile typically relies on cross-functional teams composed of various stakeholders and subject matter experts.\\n8. Decision-making: RAG involves more formalized decision-making processes than Agile, which tends to be more flexible and adaptive in nature.\\n9. Metrics and reporting: RAG typically includes specific metrics and reporting requirements for risk management, while Agile emphasizes continuous monitoring and feedback through various performance metrics, such as velocity and lead time.\\n10. Cultural fit: RAG is often seen as a more structured and formalized approach to project management, whereas Agile is generally viewed as a more flexible and adaptive methodology'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is RAG??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQxF10qT-UXc"
   },
   "source": [
    "### using **custom** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBNlpm8__Tsx"
   },
   "source": [
    "#### RecursiveCharacterTextSplitter is a text splitter that splits the text into chunks, trying to keep paragraphs togeher and avoid loosing context over pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "3PY9wEh--WVQ"
   },
   "outputs": [],
   "source": [
    "pdf_reader = PyPDFLoader(\"../data\\RAGPaper.pdf\")\n",
    "documents = pdf_reader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "iEYY47mb_fnQ"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create embeddings\n",
    "# embeddings = HuggingFaceInferenceAPIEmbeddings(api_key=HF_TOKEN,\n",
    "#                                                model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "db = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "# FAISS: Facebook AI Similarity Search --> Powerful library for similarity search and clustering of dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT=\"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
    "answering something not correct. If you don't know the answer to a question,\n",
    "please don't share false information. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow up Input: {question}\n",
    "Standalone questions: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\n",
    "template = B_INST + SYSTEM_PROMPT + instruction + E_INST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate(template=template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "S6Va-LF7-1BC"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,retriever=db.as_retriever(),condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(combine_docs_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=CTransformers(client=<ctransformers.llm.LLM object at 0x000002C77D8852A0>, model='../model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', config={'max_new_tokens': 600, 'temperature': 0.01, 'context_length': 5000})), document_variable_name='context'), question_generator=LLMChain(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template=\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of \\nanswering something not correct. If you don't know the answer to a question,\\nplease don't share false information. \\n<</SYS>>\\n\\n\\nGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\nChat History:\\n{chat_history}\\nFollow up Input: {question}\\nStandalone questions: \\n[/INST]\"), llm=CTransformers(client=<ctransformers.llm.LLM object at 0x000002C77D8852A0>, model='../model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', config={'max_new_tokens': 600, 'temperature': 0.01, 'context_length': 5000})), return_source_documents=True, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002CA568631C0>))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNsB9gehC7Gi"
   },
   "source": [
    "### Ask a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0im4CUd1CDuT",
    "outputId": "920306ab-90ad-4c98-b809-cbbf165e1e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sachin Ramesh Tendulkar (born April 24, 1973) is a former Indian cricketer and captain who is widely regarded as one of the greatest batsmen in the history of cricket. He was born in Mumbai, India, and made his first-class debut in 1989. Tendulkar scored over 34,000 runs in international cricket, including 15,921 runs in Test cricket, which is the most by any player in history. He also holds several other records, including most centuries scored in Test cricket (51) and most runs scored in a single World Cup edition (673). Tendulkar was named the ICC Cricketer of the Year in 2010, and he was awarded the Bharat Ratna, India's highest civilian honor, in 2014.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"Who is Sachin Tendulkar\"\"\"\n",
    "result = qa({\"question\":query,\"chat_history\":chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUgPaRiJEN5H",
    "outputId": "1ec5a76d-36cf-4cf2-98dc-0c77da67156c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAGs stands for Retrieval-based Autoencoder with Generative modeling. It's a type of neural network architecture that combines the strengths of autoencoders and generative models to perform various NLP tasks such as text generation, language translation, and question answering.\n",
      "\n",
      "RAGs consist of two main components: a retriever and a generator. The retriever takes in a query and outputs a distribution over text passages, which are then used as additional context for the generator when generating the target sequence. This allows the generator to generate more informative and relevant output given the input query.\n",
      "\n",
      "One of the main use cases of RAGs is in language translation tasks. By using a retriever to retrieve relevant passages from a parallel corpus, the generator can learn to translate between languages more accurately. Another use case is in question answering, where the retriever can retrieve relevant passages from a large document collection and the generator can generate answers to questions based on the retrieved passages.\n",
      "\n",
      "RAGs have several advantages over traditional neural network architectures. Firstly, they can handle long-range dependencies more effectively since the generator has access to the entire input sequence when generating each element of the output sequence. Secondly, they can capture complex contextual relationships between elements in the input sequence, leading to more informative and relevant output. Finally, they are relatively lightweight compared to other neural network architectures, making them a good choice for tasks where computational resources are limited.\n",
      "\n",
      "In summary, RAGs are a powerful tool for NLP tasks that leverage contextual information from large document collections. They have shown promising results in language translation and question answering tasks and have the potential to be applied to other NLP tasks as well.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"What is RAGs and tell me more about use cases of RAGs, in a detailed manner\"\"\"\n",
    "result = qa.invoke({\"question\":query,\"chat_history\":chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
