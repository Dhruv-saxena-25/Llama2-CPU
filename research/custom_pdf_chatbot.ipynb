{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7h0mK4Wg7aw4"
   },
   "source": [
    "### Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtJApIy375_B",
    "outputId": "5b15891c-4bad-4ff1-a306-8a74b8388401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q langchain_community\n",
    "!pip install -q sentence_transformers\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeXgHqSp7-_B"
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F3cbHyRc78hh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model= \"../model\\llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                    model_type= 'llama',\n",
    "                    config={'max_new_tokens': 600,\n",
    "                              'temperature': 0.01,\n",
    "                              'context_length': 5000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer('BAAI/bge-base-en-v1.5', cache_folder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path= \"../models--BAAI--bge-base-en-v1.5\\\\snapshots\\\\a5beb1e3e68b9ab74eb54cfd186867f64f240e1a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with model configuration options, specifying to use the CPU/GPU for computations\n",
    "model_kwargs = {'device':'cuda'} #model_kwargs = {'device':'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv Saxena\\AppData\\Local\\Temp\\ipykernel_113512\\1380460386.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_path,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Unterscheidung between RAG and Agile?\\nRAG stands for \"Risks, Assumptions, and Gates\". It is a tool used in project management to identify, track, and manage risks, assumptions, and milestones in a project. RAG status is typically used in conjunction with Agile methodologies, but it can also be used in other project management frameworks.\\n\\nRAG is often used in Agile projects to help teams prioritize and manage their work. It provides a simple and visual way to categorize tasks based on their level of risk or uncertainty. Tasks are assigned a RAG status, which can be either Green (low risk), Amber (medium risk), or Red (high risk). This helps teams identify the most critical tasks and allocate resources accordingly.\\n\\nHere are some key differences between RAG and Agile:\\n\\n1. Focus: RAG is focused specifically on risk management, while Agile is a broader project management framework that encompasses various aspects of project delivery, including planning, execution, and monitoring.\\n2. Scope: RAG is typically used in conjunction with Agile methodologies, whereas Agile can be used independently or in combination with other frameworks.\\n3. Level of detail: RAG provides a high level of detail in terms of risk categorization, while Agile offers more flexibility and adaptability in terms of project planning and execution.\\n4. Timeframe: RAG is typically used on a per-project basis, whereas Agile is often applied over longer periods, such as sprints or iterations.\\n5. Tools and techniques: RAG relies heavily on visualization tools, such as traffic lights or stoplight charts, to communicate risk status. Agile, on the other hand, employs a range of tools and techniques, including user stories, sprint planning, and retrospectives.\\n6. Team structure: RAG typically involves a dedicated risk manager or owner, while Agile often relies on cross-functional teams with diverse skill sets and expertise.\\n7. Mindset: RAG emphasizes proactive risk management and mitigation, while Agile encourages adaptability, flexibility, and continuous improvement.\\n8. Metrics: RAG focuses on measuring risk exposure and mitigation efforts, while Agile tracks progress through metrics such as velocity, lead time, and cycle time.\\n9. Reporting: RAG typically involves regular reporting to stakeholders on risk status and mitigation efforts, while Agile often relies on more frequent feedback and inspection points throughout the project lifecycle.\\n10. Culture: RAG fosters a culture of proactive risk management and mitigation, while Agile encourages a culture of collaboration, continuous improvement, and customer satisfaction.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is RAG??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQxF10qT-UXc"
   },
   "source": [
    "### using **custom** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBNlpm8__Tsx"
   },
   "source": [
    "#### RecursiveCharacterTextSplitter is a text splitter that splits the text into chunks, trying to keep paragraphs togeher and avoid loosing context over pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3PY9wEh--WVQ"
   },
   "outputs": [],
   "source": [
    "pdf_reader = PyPDFLoader(\"../data\\RAGPaper.pdf\")\n",
    "documents = pdf_reader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iEYY47mb_fnQ"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create embeddings\n",
    "# embeddings = HuggingFaceInferenceAPIEmbeddings(api_key=HF_TOKEN,\n",
    "#                                                model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "db = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "# FAISS: Facebook AI Similarity Search --> Powerful library for similarity search and clustering of dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow up Input: {question}\n",
    "Standalone questions: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S6Va-LF7-1BC"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,retriever=db.as_retriever(),condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(combine_docs_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=CTransformers(client=<ctransformers.llm.LLM object at 0x0000020C4DFE0D60>, model='../model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', config={'max_new_tokens': 600, 'temperature': 0.01, 'context_length': 5000})), document_variable_name='context'), question_generator=LLMChain(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\nChat History:\\n{chat_history}\\nFollow up Input: {question}\\nStandalone questions: '), llm=CTransformers(client=<ctransformers.llm.LLM object at 0x0000020C4DFE0D60>, model='../model\\\\llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', config={'max_new_tokens': 600, 'temperature': 0.01, 'context_length': 5000})), return_source_documents=True, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020E78991300>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNsB9gehC7Gi"
   },
   "source": [
    "### Ask a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0im4CUd1CDuT",
    "outputId": "920306ab-90ad-4c98-b809-cbbf165e1e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sachin Ramesh Tendulkar (born April 24, 1973) is a former Indian cricketer and captain who is widely regarded as one of the greatest batsmen in the history of cricket. He was born in Mumbai, India, and made his first-class debut in 1989. Tendulkar scored over 34,000 runs in international cricket, including 15,921 runs in Test cricket, which is the most by any player in history. He also holds several other records, including most centuries scored in Test cricket (51) and most runs scored in a single World Cup edition (673). Tendulkar was named the ICC Cricketer of the Year in 2010, and he was awarded the Bharat Ratna, India's highest civilian honor, in 2014.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"Who is Sachin Tendulkar\"\"\"\n",
    "result = qa({\"question\":query,\"chat_history\":chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUgPaRiJEN5H",
    "outputId": "1ec5a76d-36cf-4cf2-98dc-0c77da67156c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAGs stands for Retrieval-based Autoencoder with Generative model, which is a type of neural network architecture that combines the strengths of both retrieval-based models and generative models. The basic idea behind RAGs is to use a retriever to retrieve relevant text documents from a large corpus, and then use these documents as additional context when generating the target sequence. This allows the model to learn how to generate high-quality text that is relevant to the input sequence, rather than simply relying on generic language models.\n",
      "\n",
      "One of the main advantages of RAGs is that it can be used for a wide range of natural language processing tasks, such as text generation, language translation, and question answering. For example, in text generation, RAGs can be trained to generate coherent and contextually relevant text by using the retrieved documents as additional context. In language translation, RAGs can be used to translate text from one language to another while also taking into account the context of the input text. And in question answering, RAGs can be used to generate answers to questions based on the content of the retrieved documents.\n",
      "\n",
      "Another advantage of RAGs is that it can be easily combined with other techniques, such as pre-training and fine-tuning, to further improve its performance. For example, a RAGs model can be pre-trained on a large corpus of text data, and then fine-tuned on a specific task or dataset to adapt to the task at hand. This allows the model to learn both general language knowledge and task-specific knowledge, leading to better performance on the target task.\n",
      "\n",
      "In terms of use cases, RAGs can be applied to a wide range of applications, such as:\n",
      "\n",
      "* Text generation: RAGs can be used to generate coherent and contextually relevant text, such as chatbots, automated customer service agents, or content generation for websites.\n",
      "* Language translation: RAGs can be used to translate text from one language to another while also taking into account the context of the input text.\n",
      "* Question answering: RAGs can be used to generate answers to questions based on the content of the retrieved documents.\n",
      "* Summarization: RAGs can be used to summarize long documents or articles, extracting the most important information and generating a concise summary.\n",
      "* Creative writing: RAGs can be used to generate creative writing, such as poetry or short stories, by combining the retrieved documents with generic language models.\n",
      "\n",
      "Overall, RAGs is a powerful tool for natural language processing tasks that combines the strengths of both retrieval-based models and generative models. Its ability to learn from large amounts of text data and generate high-quality text makes it a promising approach for a wide range of applications.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"What is RAGs and tell me more about use cases of RAGs, in a detailed manner\"\"\"\n",
    "result = qa.invoke({\"question\":query,\"chat_history\":chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content= query),\n",
    "        AIMessage(content=result[\"answer\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is RAGs and tell me more about use cases of RAGs, in a detailed manner'),\n",
       " AIMessage(content=' RAGs stands for Retrieval-based Autoencoder with Generative model, which is a type of neural network architecture that combines the strengths of both retrieval-based models and generative models. The basic idea behind RAGs is to use a retriever to retrieve relevant text documents from a large corpus, and then use these documents as additional context when generating the target sequence. This allows the model to learn how to generate high-quality text that is relevant to the input sequence, rather than simply relying on generic language models.\\n\\nOne of the main advantages of RAGs is that it can be used for a wide range of natural language processing tasks, such as text generation, language translation, and question answering. For example, in text generation, RAGs can be trained to generate coherent and contextually relevant text by using the retrieved documents as additional context. In language translation, RAGs can be used to translate text from one language to another while also taking into account the context of the input text. And in question answering, RAGs can be used to generate answers to questions based on the content of the retrieved documents.\\n\\nAnother advantage of RAGs is that it can be easily combined with other techniques, such as pre-training and fine-tuning, to further improve its performance. For example, a RAGs model can be pre-trained on a large corpus of text data, and then fine-tuned on a specific task or dataset to adapt to the task at hand. This allows the model to learn both general language knowledge and task-specific knowledge, leading to better performance on the target task.\\n\\nIn terms of use cases, RAGs can be applied to a wide range of applications, such as:\\n\\n* Text generation: RAGs can be used to generate coherent and contextually relevant text, such as chatbots, automated customer service agents, or content generation for websites.\\n* Language translation: RAGs can be used to translate text from one language to another while also taking into account the context of the input text.\\n* Question answering: RAGs can be used to generate answers to questions based on the content of the retrieved documents.\\n* Summarization: RAGs can be used to summarize long documents or articles, extracting the most important information and generating a concise summary.\\n* Creative writing: RAGs can be used to generate creative writing, such as poetry or short stories, by combining the retrieved documents with generic language models.\\n\\nOverall, RAGs is a powerful tool for natural language processing tasks that combines the strengths of both retrieval-based models and generative models. Its ability to learn from large amounts of text data and generate high-quality text makes it a promising approach for a wide range of applications.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
